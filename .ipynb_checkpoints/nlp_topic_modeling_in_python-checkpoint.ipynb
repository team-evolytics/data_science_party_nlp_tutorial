{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44b094b",
   "metadata": {},
   "source": [
    "# NLP Topic Modeling in Python with Non-negative Matrix Factorization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d506ab",
   "metadata": {},
   "source": [
    "## Using Jupyter Notebooks \n",
    "\n",
    "Jupyter Notebooks is an interactive Python environment for data science.   Cells are seperated into Markdown (i.e., text) and code cells.  In this notebook, you should not need to edit code (unless you really want to!). Therefore, you can just run each cell by highlighting it and pressing \"Cmd + Return\" or using the \"> Run\" key at the top.\n",
    "\n",
    "Note: A best practice is to import packages in the first cell of the notebook.  However, given that this is a tutorial I will import them in the first cell in which they are used to more closely associate the package with it's use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba32a3",
   "metadata": {},
   "source": [
    "## Installing Packages \n",
    "First, we'll install some packages we'll use today for our text manipulation and topic modeling. This may produce a lot of output so please be patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b359ee",
   "metadata": {},
   "source": [
    "## 1. Import our Data\n",
    "\n",
    "We're going to use pandas to import and inspect our data.  Notice that our text column is already in lower case and contains the article text from wikipedia.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf425424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URI                 name  \\\n",
       "0        <http://dbpedia.org/resource/Digby_Morrell>        Digby Morrell   \n",
       "1       <http://dbpedia.org/resource/Alfred_J._Lewy>       Alfred J. Lewy   \n",
       "2        <http://dbpedia.org/resource/Harpdog_Brown>        Harpdog Brown   \n",
       "3  <http://dbpedia.org/resource/Franz_Rottensteiner>  Franz Rottensteiner   \n",
       "4               <http://dbpedia.org/resource/G-Enka>               G-Enka   \n",
       "\n",
       "                                                text  \n",
       "0  digby morrell born 10 october 1979 is a former...  \n",
       "1  alfred j lewy aka sandy lewy graduated from un...  \n",
       "2  harpdog brown is a singer and harmonica player...  \n",
       "3  franz rottensteiner born in waidmannsfeld lowe...  \n",
       "4  henry krvits born 30 december 1974 in tallinn ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URI                 name  \\\n",
       "0        <http://dbpedia.org/resource/Digby_Morrell>        Digby Morrell   \n",
       "1       <http://dbpedia.org/resource/Alfred_J._Lewy>       Alfred J. Lewy   \n",
       "2        <http://dbpedia.org/resource/Harpdog_Brown>        Harpdog Brown   \n",
       "3  <http://dbpedia.org/resource/Franz_Rottensteiner>  Franz Rottensteiner   \n",
       "4               <http://dbpedia.org/resource/G-Enka>               G-Enka   \n",
       "\n",
       "                                                text  \n",
       "0  digby morrell born 10 october 1979 is a former...  \n",
       "1  alfred j lewy aka sandy lewy graduated from un...  \n",
       "2  harpdog brown is a singer and harmonica player...  \n",
       "3  franz rottensteiner born in waidmannsfeld lowe...  \n",
       "4  henry krvits born 30 december 1974 in tallinn ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data \n",
    "import pandas as pd \n",
    "\n",
    "url = 'https://raw.githubusercontent.com/team-evolytics/data_science_party_nlp_tutorial/main/people_wiki.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "#Inspect our dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425a27f",
   "metadata": {},
   "source": [
    "## 2. Pre-process our Text \n",
    "\n",
    "Before we analyze our text we need to clean it.  Cleaning text involves standardizing and removing terms that are non-informative.  Terms that occur in most documents or, alternatively, very few, are unlikely to help us know how to group things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# These lines determines what the punctuation and numbers are replaced with.\n",
    "punct_table = str.maketrans({ch: ' ' for ch in string.punctuation})  \n",
    "digit_table = str.maketrans('', '', string.digits)\n",
    "\n",
    "# Text cleaning functions\n",
    "remove_punctuation = lambda x: x.translate(punct_table)\n",
    "remove_numbers = lambda x: x.translate(digit_table)\n",
    "remove_urls = lambda x: re.sub(r\"http\\S+\", \"\", x)\n",
    "\n",
    "\n",
    "#Tokenize texts.  Note- It is possible to comment out steps with a # to change how tokenization occurs. \n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes a list of strings and return \n",
    "         \n",
    "    \"\"\"\n",
    "    # Creates stopword list from NLTK.\n",
    "    sw = stopwords.words(\"english\") + ['']\n",
    "    \n",
    "    # Creates a tokenizer instance. \n",
    "    tokenizer = WordPunctTokenizer() \n",
    "    \n",
    "    # Text cleaning. \n",
    "    text = remove_urls(text) # removes urls \n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text) # removes numbers.  Leaving here as dates may be informative. \n",
    "    text = text.lower() # sets to lowercase\n",
    "\n",
    "    # Tokenization \n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tkns = tokenizer.tokenize(text) # tokenizes text\n",
    "\n",
    "    # Remove stopwords \n",
    "    tokenized_text = [tkn for tkn in tkns if tkn not in sw]\n",
    "\n",
    "    return tokenized_text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78017d5",
   "metadata": {},
   "source": [
    "Let's see a demonstration of what the above function is doing! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I moved to Kansas City in 2020 during the pandemic!  It was hard to find housing \" \\\n",
    "       \"with social distancing but I was eventually able to find one on Zillow (http://www.zillow.com).\"\n",
    "\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e7b47",
   "metadata": {},
   "source": [
    "### Feature Extraction (Vectorization) \n",
    "\n",
    "Computers only undestand numbers we need to convert the tokenized documents into vectors.  To do this we'll use term frequency - inverse docuemnt frequency (TF-IDF) metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, max_df=.95, min_df=.0001, ngram_range=(1,2))\n",
    "doc_term_matrix = vectorizer.fit_transform(list(df['text']))\n",
    "\n",
    "print(\"Our matrix has {} documents and {} vocbaulary terms.\".format(doc_term_matrix.shape[0], \n",
    "                                                                    doc_term_matrix.shape[1]))\n",
    "\n",
    "# Store our vocab for later use. \n",
    "model_features = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Note that our model features list has the same length as the vocabulary (i.e, they are the same.): {} \\n\".format(len(model_features)))\n",
    "print(\"Sample Feature Names: \", model_features[100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350412f",
   "metadata": {},
   "source": [
    "# 3.  Model Creation:  Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "Shout out to Hui for teaching me that this could be used for topic modeling!  We are going to keep things very simple but I want to provide just a superficial explanation of the logic of this technique.  \n",
    "\n",
    "Non-negative matrix factorization (NMF) seems like an intimidating technique but the basic logic is very simple. You may remember from elementary school that factorization is breaking a number down into numbers that when multiplied together equal the initial value (e.g., 30 = (2X3)X5 ).  We can do something similiar with matrices.  However, matrix multiplication has a special requirement that the number of rows in one matrix (we'll call is matrix W) must equal the number of columns in the other matrix (matrix H).  Because there are many different values that the number of rows/number of columns can take on we have to select a value for factorization.  Matrix multiplication follows slighty different rules from traditional multiplication which you can read about here.    \n",
    "\n",
    "Just like we can multiply a simple factorization back together to get the original value (e.g., (2x3)x5 = 30), we can multiply Matrix W X Matrix H to reconstruct our original Document X Term matrix.  However, our reconstructed matrix is unlikely to exactly match our original matrix and the degree to which the values differ tells us how well our proposed model fits.  \n",
    "\n",
    "One last thing!  *Non-negative Matrix Factorization is exactly that - non-negative*.  No value in the matrix can be below zero.  This makes it a good fit for behavioral data (you can't have negative clicks) but a poor fit for things like financial data where there may be negative values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68377d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the number of topics to model. \n",
    "num_topics = 100\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Initialize model\n",
    "model = NMF(init='nndsvd', n_components=num_topics)\n",
    "\n",
    "# Fit our corpus to the model \n",
    "model.fit(doc_term_matrix)\n",
    "\n",
    "# Get document weights for each component. \n",
    "doc_weights = model.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54c6c3",
   "metadata": {},
   "source": [
    "# 4.  Model Inspection \n",
    "\n",
    "### Retrieve Top Terms for each Topic\n",
    "\n",
    "Here we are retrieving the topic terms that most characterize each topic.  Note that all terms are scored for each topic but we are interested in terms that are most unique and typical for a given a topic.   Generally we inspect the top N terms to get an idea about what the topic is.  I've provided an easy way for you to vary the number of terms returned below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f59374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many terms to do you want to retrieve? \n",
    "n_terms = 10\n",
    "\n",
    "def get_nmf_topics(model, n_top_words, num_topics, feat_names):\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-n_top_words - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict)\n",
    "\n",
    "nmf_topics = get_nmf_topics(model, n_terms, model.n_components, model_features)\n",
    "nmf_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99e279",
   "metadata": {},
   "source": [
    "### Inspecting Representative Documents\n",
    "\n",
    "We typically want to be able to inspect the original data.  Above when we fit the model, we saved weights that each component characterizes a document.  We're going to select the component with the max value for each document and assign it to that topic.  Then we'll filter our dataframe by topic to see if our topic modeling worked! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Note that the document weights matrix has the name number of rows as documents: {}\".format(doc_weights.shape[0]))\n",
    "print(\"Additionally observe that is the same number of columns as our topics: {}\".format(doc_weights.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf2537",
   "metadata": {},
   "source": [
    "Now let's print some bios to inspect! Do they make sense to you?  What themes do they have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5523c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which topics documents do you want to inspect? \n",
    "topic_id =47\n",
    "\n",
    "# Reformating topic number \n",
    "topic_col = 'Topic # ' + '{:02d}'.format(topic_id)\n",
    "\n",
    "# Get topic terms \n",
    "print(\"These words characterize this topic: \", \"\\n\")\n",
    "print(nmf_topics[topic_col], \"\\n\\n\")\n",
    "\n",
    "# Assign topics to biographies in the Dataframe\n",
    "df[\"Topic_idx\"] = doc_weights.argmax(axis=1)\n",
    "\n",
    "# Filtering our dataframe. \n",
    "df_topic = df.loc[df['Topic_idx'] == topic_id] \n",
    "bios = zip(df_topic['name'], df_topic['text'])\n",
    "\n",
    "# Displaying the selected bios. \n",
    "print(\"Here are the biographies for individuals who scored highly on this topic: \", '\\n')\n",
    "\n",
    "for bio in bios:\n",
    "    print(\"Name: \", bio[0])\n",
    "    print(\"Biography: \", bio[1], \"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67235654",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "Hopefully you found the tutorial above interesting!  If you want to learn more about cleaning and preprocessing text as well as a different technique for topic modeling check out the Evolytics blog series here:  \n",
    "\n",
    "- [Part II. Preparing Text for Analysis with Natural Language Toolkit (NLTK)](https://evolytics.com/blog/open-ended-survey-questions-for-computational-analysis-part-ii/)\n",
    "- [Part III. How to Find Near Duplicate Text and Recognize Name Entities in Survey Responses](https://evolytics.com/blog/survey-responses-duplicate-text-and-named-entities/)\n",
    "\n",
    "Please feel free to use the above code for your own projects.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
