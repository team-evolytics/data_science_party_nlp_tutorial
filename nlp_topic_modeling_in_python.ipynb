{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44b094b",
   "metadata": {},
   "source": [
    "# NLP Topic Modeling in Python with Non-negative Matrix Factorization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d506ab",
   "metadata": {},
   "source": [
    "# Preface - Installing Packages \n",
    "\n",
    "## Using Jupyter Notebooks \n",
    "\n",
    "Jupyter Notebooks is an interactive Python environment for data science.   Cells are seperated into Markdown (i.e., text) and code cells.  In this notebook, you should not need to edit code (unless you really want to!). Therefore, you can just run each cell by highlighting it and pressing \"Cmd + Return\" or using the \"> Run\" key at the top.\n",
    "\n",
    "Note: A best practice is to import packages in the first cell of the notebook.  However, given that this is a tutorial I will import them in the first cell in which they are used to more closely associate the package with it's use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba32a3",
   "metadata": {},
   "source": [
    "## Installing Packages \n",
    "First, we'll install some packages we'll use today for our text manipulation and topic modeling. This may produce a lot of output so please be patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not present. \n",
    "!pip install nltk\n",
    "!pip install squarify\n",
    "\n",
    "# Import libraries into our environment. \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b359ee",
   "metadata": {},
   "source": [
    "## 1. Import our Data\n",
    "\n",
    "We're going to use pandas to import and inspect our data.  Notice that our text column is already in lower case and contains the article text from wikipedia.  This is not typically the case in real world data but it saves us some time effort in pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf425424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "import pandas as pd \n",
    "\n",
    "filename = 'people_wiki.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "#Inspect our dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425a27f",
   "metadata": {},
   "source": [
    "## 2. Pre-process our Text \n",
    "\n",
    "Before we analyze text we need to clean it.  Cleaning text involves standardizing and removing terms that are non-informative.  Terms that occur in most documents or, alternatively, very few, are unlikely to help us know how to group things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# These lines determines what the punctuation and numbers are replaced with.\n",
    "punct_table = str.maketrans({ch: ' ' for ch in string.punctuation})  \n",
    "digit_table = str.maketrans('', '', string.digits)\n",
    "\n",
    "# Text cleaning functions\n",
    "remove_punctuation = lambda x: x.translate(punct_table)\n",
    "remove_numbers = lambda x: x.translate(digit_table)\n",
    "remove_urls = lambda x: re.sub(r\"http\\S+\", \"\", x)\n",
    "\n",
    "\n",
    "#Tokenize texts.  Note- It is possible to comment out steps with a # to change how tokenization occurs. \n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes a list of strings and return \n",
    "         \n",
    "    \"\"\"\n",
    "    # Creates stopword list from NLTK.\n",
    "    sw = stopwords.words(\"english\") + ['']\n",
    "      \n",
    "    # Text cleaning. \n",
    "    text = remove_urls(text) # removes urls \n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text) # removes numbers.  Leaving here as dates may be informative. \n",
    "    text = text.lower() # sets to lowercase\n",
    "    \n",
    "    # Tokenization \n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tkns = tokenizer.tokenize(text) # tokenizes text\n",
    "\n",
    "    # Remove stopwords \n",
    "    tokenized_text = [tkn for tkn in tkns if tkn not in sw]\n",
    "\n",
    "    return tokenized_text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78017d5",
   "metadata": {},
   "source": [
    "Let's see a demonstration of what the above function is doing! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I moved to Kansas City in 2020 during the pandemic!  It was hard to find housing \" \\\n",
    "       \"with social distancing but I was eventually able to find one on Zillow (http://www.zillow.com).\"\n",
    "\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ed463",
   "metadata": {},
   "source": [
    "### Splitting Data \n",
    "\n",
    "We're not going to try to optimize the fit of the model in this tutorial but I do want to demonstrate how you can fit unseen data to the model.  Therefore, we're still going to do a test train split.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split our data. \n",
    "df_train, df_test = train_test_split(df, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e7b47",
   "metadata": {},
   "source": [
    "### Feature Extraction (Vectorization) \n",
    "\n",
    "Computers only undestand numbers we need to convert the tokenized documents into vectors.  To do this we'll use term frequency - inverse docuemnt frequency (TF-IDF) metric. \n",
    "\n",
    "Note that we only fit_transform our training data, not our unseen test data.  This helps prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Intialize vectorizer and transform/vectorize the corpus. Note we override the tokenizer with our custom one. \n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, max_df=.95, min_df=.001, ngram_range=(1,2))\n",
    "doc_term_matrix = vectorizer.fit_transform(list(df_train['text']))\n",
    "\n",
    "print(\"Our matrix has {} documents and {} vocabulary terms.\".format(doc_term_matrix.shape[0], \n",
    "                                                                    doc_term_matrix.shape[1]))\n",
    "\n",
    "# Store our vocab for later use. \n",
    "model_features = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Note that our model features list has the same length as the vocabulary (i.e, they are the same.): {} \\n\".format(len(model_features)))\n",
    "print(\"Sample Feature Names: \", model_features[100:110], '\\n')\n",
    "print(\"A document is represented like this: \\n\", doc_term_matrix[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350412f",
   "metadata": {},
   "source": [
    "# 3.  Model Creation:  Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "Shout out to Hui for teaching me that this could be used for topic modeling!  We are going to keep things very simple but I want to provide just a superficial explanation of the logic of this technique.  \n",
    "\n",
    "Non-negative matrix factorization (NMF) seems like an intimidating technique but the basic logic is very simple. You may remember from elementary school that factorization is breaking a number down into numbers that when multiplied together equal the initial value (e.g., 30 = (2X3)X5 ).  We can do something similiar with matrices.  However, matrix multiplication has a special requirement that the number of rows in one matrix (we'll call is matrix W) must equal the number of columns in the other matrix (matrix H).  Because there are many different values that the number of rows/number of columns can take on we have to select a value for factorization.  Matrix multiplication follows slighty different rules from traditional multiplication which you can read about here.    \n",
    "\n",
    "Just like we can multiply a simple factorization back together to get the original value (e.g., (2x3)x5 = 30), we can multiply Matrix W X Matrix H to reconstruct our original Document X Term matrix.  However, our reconstructed matrix is unlikely to exactly match our original matrix and the degree to which the values differ tells us how well our proposed model fits.  \n",
    "\n",
    "One last thing!  *Non-negative Matrix Factorization is exactly that - non-negative*.  No value in the matrix can be below zero.  This makes it a good fit for behavioral data (you can't have negative clicks) but a poor fit for things like financial data where there may be negative values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68377d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Enter the number of topics to model. \n",
    "num_topics = 100\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Initialize model\n",
    "model = NMF(init='nndsvda', n_components=num_topics)\n",
    "\n",
    "# Fit our corpus to the model \n",
    "model.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54c6c3",
   "metadata": {},
   "source": [
    "# 4.  Model Inspection \n",
    "\n",
    "### Retrieve Top Terms for each Topic\n",
    "\n",
    "Here we are retrieving the topic terms that most characterize each topic.  Note that all terms are scored for each topic but we are interested in terms that are most unique and typical for a given a topic.   Generally we inspect the top N terms to get an idea about what the topic is.  I've provided an easy way for you to vary the number of terms returned below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f59374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many terms to do you want to retrieve? \n",
    "n_terms = 10\n",
    "\n",
    "def get_nmf_topics(model, n_top_words, num_topics, feat_names):\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-n_top_words - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict)\n",
    "\n",
    "nmf_topics = get_nmf_topics(model, n_terms, model.n_components, model_features)\n",
    "nmf_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99e279",
   "metadata": {},
   "source": [
    "### Inspecting Representative Documents\n",
    "\n",
    "We typically want to be able to inspect the original data.  Above when we fit the model, we saved weights that each component characterizes a document.  We're going to select the component with the max value for each document and assign it to that topic.  Then we'll filter our dataframe by topic to see if our topic modeling worked! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document weights for each component. \n",
    "doc_weights = model.transform(doc_term_matrix)\n",
    "\n",
    "print(\"Note that the document weights matrix has the name number of rows as documents: {}\".format(doc_weights.shape[0]))\n",
    "print(\"Additionally observe that is the same number of columns as our topics: {}\".format(doc_weights.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf2537",
   "metadata": {},
   "source": [
    "Now let's print some bios to inspect! Do they make sense to you?  What themes do they have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5523c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which topics documents do you want to inspect? \n",
    "topic_id =92\n",
    "\n",
    "# Reformating topic number \n",
    "topic_col = 'Topic # ' + '{:02d}'.format(topic_id)\n",
    "\n",
    "# Get topic terms \n",
    "print(\"These words characterize this topic: \", \"\\n\")\n",
    "print(nmf_topics[topic_col], \"\\n\\n\")\n",
    "\n",
    "# Assign topics to biographies in the Dataframe\n",
    "df_train[\"Topic_idx\"] = doc_weights.argmax(axis=1)\n",
    "\n",
    "# Filtering our dataframe. \n",
    "df_topic = df_train.loc[df_train['Topic_idx'] == topic_id] \n",
    "bios = zip(df_topic['name'], df_topic['text'])\n",
    "\n",
    "# Displaying the selected bios. \n",
    "print(\"Here are the biographies for individuals who scored highly on this topic: \", '\\n')\n",
    "\n",
    "for bio in bios:\n",
    "    print(\"Name: \", bio[0])\n",
    "    print(\"Biography: \", bio[1], \"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72aa6fa",
   "metadata": {},
   "source": [
    "# 5.  Categorizing Unseen Data \n",
    "\n",
    "Now we'll categorize some unseen data.  Note that we only call transform - both for the vectorizer and for the NMF model. This is because we've already fit the training data and we want to transform it with respect to the learned model from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b480ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we call transform not fit_transform.\n",
    "doc_term_matrix_test = vectorizer.transform(list(df_test['text']))\n",
    "\n",
    "# Predicting topic for unseen data. \n",
    "test_nmf_transformed = model.transform(doc_term_matrix_test)\n",
    "predicted_topic = test_nmf_transformed.argmax(axis=1)\n",
    "\n",
    "# Join to df_test\n",
    "df_test['Topic_idx'] = predicted_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b0b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id = 92\n",
    "\n",
    "# Filtering our dataframe. \n",
    "df_topic = df_test.loc[df_test['Topic_idx'] == topic_id] \n",
    "bios = zip(df_topic['name'], df_topic['text'])\n",
    "\n",
    "# Displaying the selected bios. \n",
    "print(\"Here are the biographies for individuals who scored highly on this topic: \", '\\n')\n",
    "\n",
    "for bio in bios:\n",
    "    print(\"Name: \", bio[0])\n",
    "    print(\"Biography: \", bio[1], \"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191c306",
   "metadata": {},
   "source": [
    "# 6. Visualization of Results \n",
    "\n",
    "There are many ways to segment and visualize the outputs of results.  Here we use a treemap to give us an idea of the prevalence of specific topics in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc8982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import squarify\n",
    "\n",
    "sns.set_style(style=\"whitegrid\") # set seaborn plot style\n",
    "sizes= df_train['Topic_idx'].value_counts()\n",
    "label= sizes.index\n",
    "squarify.plot(sizes=sizes, label=label, alpha=0.6).set(title='Treemap of Topic Prevalance')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67235654",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "Hopefully you found the tutorial above interesting!  If you want to learn more about cleaning and preprocessing text as well as a different technique for topic modeling [check out the Evolytics blog series here](https://github.com/team-evolytics/text_mining_blog).   \n",
    "\n",
    "For different implemented methods for topic modeling check out \n",
    "- [Truncated SVD (Latent Semantic Analysis (LSA)](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)\n",
    "- [Latent Dirichlet Allocation (LDA)](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html) \n",
    "\n",
    "Please feel free to use the above code for your own projects.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d71f9f",
   "metadata": {},
   "source": [
    "# Appendix I - Model taking too long to train? \n",
    "\n",
    "Having trouble getting the model to run in the time allotted for the tutorial? Fortunately, Python has a module \"pickle\" that allows for the storage of objects.  I've written a version of the model featured in this notebook to file that you can read it in to finish our exercise. \n",
    "\n",
    "**NOTE:** You must run all cells up to step 3 (i.e., fitting the model) for this to run properly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "files = [\n",
    "'nmf_model.pkl', \n",
    "'doc_term_matrix.pkl', \n",
    "'model_features.pkl', \n",
    "'vectorizer.pkl', \n",
    "'df_train.pkl',\n",
    "'df_test.pkl'\n",
    "]\n",
    "\n",
    "# Set file path. \n",
    "path = os.getcwd()+\"/saved_model/\"\n",
    "\n",
    "model_data = {}\n",
    "\n",
    "# Read in pickle file. \n",
    "for f in files: \n",
    "    with open(path+f, 'rb') as file:\n",
    "        model_data[f] = pickle.load(file)\n",
    "\n",
    "# Assign objects \n",
    "df_train = model_data['df_train.pkl']  \n",
    "df_test = model_data['df_test.pkl']\n",
    "vectorizer = model_data['vectorizer.pkl']\n",
    "model_features = model_data['model_features.pkl'] \n",
    "doc_term_matrix = model_data['doc_term_matrix.pkl']\n",
    "model = model_data['nmf_model.pkl']   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c43a4",
   "metadata": {},
   "source": [
    "# Appendix II - Want to save your model? \n",
    "\n",
    "Have you tweaked the above script and want to save your own model to file? Run the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os \n",
    "\n",
    "# Specify objects to save to file.\n",
    "files = {\n",
    "'nmf_model.pkl': model, \n",
    "'doc_term_matrix.pkl': doc_term_matrix, \n",
    "'model_features.pkl': model_features, \n",
    "'vectorizer.pkl': vectorizer, \n",
    "'df_train.pkl': df_train,\n",
    "'df_test.pkl': df_test\n",
    "}\n",
    "\n",
    "# Set file path. \n",
    "path = os.getcwd()+\"/saved_model/\"\n",
    "\n",
    "# Write objects to file. \n",
    "for k,v in files.items(): \n",
    "    with open(path+k, 'wb') as file:\n",
    "        pickle.dump(v, file)\n",
    "        \n",
    "df_train.to_csv('df_train.csv') \n",
    "df_test.to_csv('df_test.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
